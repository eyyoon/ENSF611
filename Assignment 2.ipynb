{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 2: Linear Models and Validation Metrics (30 marks total)\n",
    "### Due: October 10 at 11:59pm\n",
    "\n",
    "### Name: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will need to write code that uses linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6de86",
   "metadata": {},
   "source": [
    "## Part 1: Classification (14.5 marks total)\n",
    "\n",
    "You have been asked to develop code that can help the user determine if the email they have received is spam or not. Following the machine learning workflow described in class, write the relevant code in each of the steps below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3c6fc8",
   "metadata": {},
   "source": [
    "### Step 0: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "33f86925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d33a8",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (1 mark)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/spam.html\n",
    "\n",
    "Use the yellowbrick function `load_spam()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print the size and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "33583c67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X: (4600, 57), Type of X: <class 'pandas.core.frame.DataFrame'>\n",
      "Size of y: (4600,), Type of y: <class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: is_spam, dtype: int64"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: Import spam dataset from yellowbrick library\n",
    "from yellowbrick.datasets.loaders import load_spam\n",
    "\n",
    "# TO DO: Print size and type of X and y\n",
    "X, y = load_spam()\n",
    "print(f\"Size of X: {X.shape}, Type of X: {type(X)}\")\n",
    "print(f\"Size of y: {y.shape}, Type of y: {type(y)}\")\n",
    "\n",
    "X.head()\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156db208",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (1.5 marks)\n",
    "\n",
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "4e7204f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cement    0\n",
      "slag      0\n",
      "ash       0\n",
      "water     0\n",
      "splast    0\n",
      "coarse    0\n",
      "fine      0\n",
      "age       0\n",
      "dtype: int64\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Check if there are any missing values and fill them in if necessary\n",
    "\n",
    "#check if any missing values in X\n",
    "X_nulls = X.isnull().sum().sort_values(ascending=False)\n",
    "print(X_nulls)\n",
    "\n",
    "# Fill missing values in X with 0 or 0.0\n",
    "for column in X.columns:\n",
    "    if X[column].dtype == 'int64':\n",
    "        X[column] = X[column].fillna(data.mean())\n",
    "    elif X[column].dtype == 'float64':\n",
    "        X[column] = X[column].fillna(data.mean())\n",
    "\n",
    "# Check if there are any missing values in y\n",
    "y_nulls = y.isnull().sum()\n",
    "print(y_nulls)\n",
    "\n",
    "# Fill missing values in y with 0\n",
    "if y_nulls > 0:\n",
    "    y = y.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a489285a",
   "metadata": {},
   "source": [
    "For this task, we want to test if the linear model would still work if we used less data. Use the `train_test_split` function from sklearn to create a new feature matrix named `X_small` and a new target vector named `y_small` that contain **5%** of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f9bc4a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "991     1\n",
       "2824    0\n",
       "1906    0\n",
       "1471    1\n",
       "1813    0\n",
       "       ..\n",
       "1012    1\n",
       "1855    0\n",
       "2717    0\n",
       "1300    1\n",
       "1082    1\n",
       "Name: is_spam, Length: 230, dtype: int64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: Create X_small and y_small \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_small, y_train, y_small, = train_test_split(X, y, test_size=.05, random_state=0)\n",
    "\n",
    "X_small\n",
    "y_small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6c46f",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import `LogisticRegression` from sklearn\n",
    "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
    "3. Implement the machine learning model with three different datasets: \n",
    "    - `X` and `y`\n",
    "    - Only first two columns of `X` and `y`\n",
    "    - `X_small` and `y_small`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89f3d84",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model\n",
    "\n",
    "Calculate the training and validation accuracy for the three different tests implemented in Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352106a3",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "1. Create a pandas DataFrame `results` with columns: Data size, training accuracy, validation accuracy\n",
    "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "be4b5c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Data size  Training Accuracy (All Data)  \\\n",
      "0  (4600, 57)                      0.927174   \n",
      "1   (4600, 2)                      0.614946   \n",
      "2   (230, 57)                      0.956522   \n",
      "\n",
      "   Training Accuracy (Cross-Validation)  \\\n",
      "0                              0.927785   \n",
      "1                              0.615693   \n",
      "2                              0.956527   \n",
      "\n",
      "   Validation Accuracy (Cross-Validation)  \n",
      "0                                0.922283  \n",
      "1                                0.614402  \n",
      "2                                0.902102  \n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "#Pandas DataFrame results \n",
    "results = pd.DataFrame({\"Data size\": [], \"Training Accuracy (All Data)\": [], \"Training Accuracy (Cross-Validation)\": [], \"Validation Accuracy (Cross-Validation)\": []})\n",
    "\n",
    "# Define the datasets and corresponding labels\n",
    "datasets = [(X, y, \"X and y\"), (X.iloc[:, :2], y, \"First two columns\"), (X_small, y_small, \"X_small and y_small\")]\n",
    "\n",
    "# Initialize and evaluate models for each dataset, i.e. the regular X and y, first two columns, and X_small/y_small\n",
    "for data, target, data_label in datasets:\n",
    "    \n",
    "    # Instantiate model \n",
    "    model = LogisticRegression(max_iter=2000)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=0)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    training_accuracy_all_data = model.score(X_train, y_train)\n",
    "    \n",
    "    # calculate training and validation accuracy \n",
    "    scores = cross_validate(model, X_train, y_train, cv=5, scoring=\"accuracy\", return_train_score=True)\n",
    "    training_accuracy_cv = scores[\"train_score\"].mean()\n",
    "    validation_accuracy_cv = scores[\"test_score\"].mean()\n",
    "    \n",
    "    data_shape_str = str(data.shape)\n",
    "    \n",
    "    # Add the results to the DataFrame\n",
    "    results.loc[len(results)] = [data_shape_str, training_accuracy_all_data, training_accuracy_cv, validation_accuracy_cv]\n",
    "\n",
    "\n",
    "# Print the results DataFrame\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4427d4f",
   "metadata": {},
   "source": [
    "### Questions (4 marks)\n",
    "1. How do the training and validation accuracy change depending on the amount of data used? Explain with values.\n",
    "Depending on the amount of the data used, the validation accuracy increases. For example, using the original dataset with size 4600 by 57, the validation accuracy is the highest, with 0.922 when compared to 0.614 and 0.902. This means that using the original dataset, the model generalizes to unseen data the best. On the other hand, when using two columns only for the dataset, the value is 0.614 which is quite low compared to the rest. One interesting thing to see here is that the training accuracy on a smaller dataset (5%), is actually higher than the normal dataset. This can be attuned to the fact that smaller datasets tend to be easier to fit because there is less noise and complexity in the data. \n",
    "\n",
    "2. In this case, what do a false positive and a false negative represent? Which one is worse?\n",
    "Since 1 in the target column y represents spam, a false positive represents in this case, an email as a spam when the email is not a spam email. A false negative on the other hand would represents a spam email as a normal email, i.e. we missed classifying a spam email correctly and it goes through to the inbox. In my opinion, both can have consequences. For example, if the spam email is passed through to an inbox and trade security is leaked, there is definitely more chance of a false negative being worse. However, a false positive can have also some consequences such as an important email being missed, or an important deadline. In my opinion, I think the risk of a false positive is worse since the effects may be significant (cyberattacks). \n",
    "\n",
    "int: {1 for spam, 0 for not spam}\n",
    "*YOUR ANSWERS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7559517a",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "I sourced my code using reference from ENSF 611 class notes available on D2L: https://colab.research.google.com/drive/1VGEj1carkVz06Deza4m7JoK_SW9LkseE?usp=sharing\n",
    "This lab activity notes allowed me to understand what processes to follow regarding the model regression initialization, validation and visualization. I also used the code available from D2L: https://d2l.ucalgary.ca/d2l/le/content/543310/viewContent/6091032/View\n",
    "Some of the accuracy code was derived from this worksheet in order to find the training and validation accuracy. There was also information using LogisticRegression that I applied to this code. \n",
    "\n",
    "2. In what order did you complete the steps?\n",
    "I completed the steps for this question following the step order as provided. I created a first pass of one model, i.e. X and y and got its respective accuracy results without using a for loop. Then, I created the for loop to achieve the other two models and their respective errors. \n",
    "\n",
    "3. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "I did not use generative AI to modify the code at all. I did use it however to try understading the main differences between the training accuracy using cross-validation and all data. (https://chat.openai.com/)\n",
    "\n",
    "4. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?\n",
    "I had some challenges regarding the DataFrame object and passing the values of accuracies through the usage of the for loop, but was able to figure it out. There were also some initial challenges where I could not understand how to use train_test_split to achieve X_small and y_small, but was able to test it using an output to see the difference of size.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fe687f",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4c78a8",
   "metadata": {},
   "source": [
    "## Part 2: Regression (10.5 marks total)\n",
    "\n",
    "For this section, we will be evaluating concrete compressive strength of different concrete samples, based on age and ingredients. You will need to repeat the steps 1-4 from Part 1 for this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ba83c5",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (1 mark)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
    "\n",
    "Use the yellowbrick function `load_concrete()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
    "\n",
    "Print the size and type of `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "6ff2e34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X: (1030, 8), Type of X: <class 'pandas.core.frame.DataFrame'>\n",
      "Size of y: (1030,), Type of y: <class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cement</th>\n",
       "      <th>slag</th>\n",
       "      <th>ash</th>\n",
       "      <th>water</th>\n",
       "      <th>splast</th>\n",
       "      <th>coarse</th>\n",
       "      <th>fine</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cement   slag  ash  water  splast  coarse   fine  age\n",
       "0   540.0    0.0  0.0  162.0     2.5  1040.0  676.0   28\n",
       "1   540.0    0.0  0.0  162.0     2.5  1055.0  676.0   28\n",
       "2   332.5  142.5  0.0  228.0     0.0   932.0  594.0  270\n",
       "3   332.5  142.5  0.0  228.0     0.0   932.0  594.0  365\n",
       "4   198.6  132.4  0.0  192.0     0.0   978.4  825.5  360"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: Import spam dataset from yellowbrick library\n",
    "\n",
    "from yellowbrick.datasets.loaders import load_concrete\n",
    "\n",
    "# TO DO: Print size and type of X and y\n",
    "X, y = load_concrete()\n",
    "print(f\"Size of X: {X.shape}, Type of X: {type(X)}\")\n",
    "print(f\"Size of y: {y.shape}, Type of y: {type(y)}\")\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5294cfa",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (0.5 marks)\n",
    "\n",
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "693c5fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cement    0\n",
      "slag      0\n",
      "ash       0\n",
      "water     0\n",
      "splast    0\n",
      "coarse    0\n",
      "fine      0\n",
      "age       0\n",
      "dtype: int64\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "## TO DO: Check if there are any missing values and fill them in if necessary\n",
    "\n",
    "#check if any missing values in X\n",
    "X_nulls = X.isnull().sum().sort_values(ascending=False)\n",
    "print(X_nulls)\n",
    "\n",
    "# Fill missing values in X with 0 or 0.0\n",
    "for column in X.columns:\n",
    "    if X[column].dtype == 'int64':\n",
    "        X[column] = X[column].fillna(data.mean())\n",
    "    elif X[column].dtype == 'float64':\n",
    "        X[column] = X[column].fillna(data.mean())\n",
    "\n",
    "# Check if there are any missing values in y\n",
    "y_nulls = y.isnull().sum()\n",
    "print(y_nulls)\n",
    "\n",
    "# Fill missing values in y with 0\n",
    "if y_nulls > 0:\n",
    "    y = y.fillna(0)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc60489",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model (1 mark)\n",
    "\n",
    "1. Import `LinearRegression` from sklearn\n",
    "2. Instantiate model `LinearRegression()`.\n",
    "3. Implement the machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "b5041945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-12 {color: black;}#sk-container-id-12 pre{padding: 0;}#sk-container-id-12 div.sk-toggleable {background-color: white;}#sk-container-id-12 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-12 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-12 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-12 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-12 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-12 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-12 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-12 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-12 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-12 div.sk-item {position: relative;z-index: 1;}#sk-container-id-12 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-12 div.sk-item::before, #sk-container-id-12 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-12 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-12 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-12 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-12 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-12 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-12 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-12 div.sk-label-container {text-align: center;}#sk-container-id-12 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-12 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-12\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" checked><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "linear_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de28482",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model (1 mark)\n",
    "\n",
    "Calculate the training and validation accuracy using mean squared error and R2 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "970c038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make predictions on test data\n",
    "test_predictions = linear_model.predict(X_test)\n",
    "\n",
    "# Make predictions on training data\n",
    "train_predictions = linear_model.predict(X_train)\n",
    "\n",
    "# Evaluate the model using mean squared error on training and test data\n",
    "test_mse = np.sqrt(mean_squared_error(y_test, test_predictions))\n",
    "train_mse = np.sqrt(mean_squared_error(y_train, train_predictions))\n",
    "\n",
    "# Calculate R2 score for training and test data \n",
    "train_r2 = r2_score(y_train, train_predictions)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aa7795",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (1 mark)\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: MSE and R2 score\n",
    "2. Add the accuracy results to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "88d223f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Training accuracy  Validation accuracy\n",
      "MSE               10.504547             9.779332\n",
      "R2 score           0.609071             0.636898\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "results = pd.DataFrame(\n",
    "    {\n",
    "        \"Training accuracy\": [train_mse, train_r2],\n",
    "        \"Validation accuracy\": [test_mse, test_r2],\n",
    "    },\n",
    "    index=[\"MSE\", \"R2 score\"]\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "print(results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a42bda",
   "metadata": {},
   "source": [
    "### Questions (2 marks)\n",
    "1. Did using a linear model produce good results for this dataset? Why or why not?\n",
    "\n",
    "Since the MSE are relatively high, this suggests the predictive performance of the linear model might not be accurate in producing good results. \n",
    "\n",
    "On the other hand, an R2 score of 0.636898 for the validation set indicates that the model explains approximately 63.69% of the variance in the validation data, and vice-versa for the training accuracy, and with a 63.69% compared to a perfect 100%, there is around 37% still not accounted for, suggesting that the linear model is not very accurate for this dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca0ff2f",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdb0880",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*\n",
    "1. Where did you source your code?\n",
    "I sourced some of my code from the lab 2 example provided below, as well as sites linked:  \n",
    "\n",
    "https://d2l.ucalgary.ca/d2l/le/content/543310/viewContent/6084981/View\n",
    "\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics\n",
    "\n",
    "2. In what order did you complete the steps?\n",
    "\n",
    "I completed these steps by first utilizing similar code from the previous part. Then, to validate the errors I searched up the link above and imported the errors, then predicted y using test and train predictions to get the errors. Once completed, the errors were visualied using a DataFrame. \n",
    "\n",
    "3. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "I did not use generative AI for this portion because the code and understanding it was covered in the lecture notes, as well as sources provided in D2L. \n",
    "\n",
    "4. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?\n",
    "I didn't run into many challenges in this part, as there were a lot of available content to help me understand and determine the results. I think what helped me to be successful was following the previous examples and understanding which datasets to compare and which results we needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72ac3eb",
   "metadata": {},
   "source": [
    "## Part 3: Observations/Interpretation (3 marks)\n",
    "\n",
    "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
    "\n",
    "\n",
    "Both the training and validation MSE values are relatively close, which suggests that the model is not overfitting (training MSE is not significantly lower than validation MSE). Another pattern I see in the results is that the validation MSE and R2 are better than the training, which makes sense since there are more entries of data (95%) than the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b84eed",
   "metadata": {},
   "source": [
    "## Part 4: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*\n",
    "\n",
    "I liked how we were applying the code and knowledge that we learnt in class for this assignment and it was explained it detail. We had enough resources to understand what the assignment was asking for and apply the knowledge learnt from class. Something that I found a little confusing was that the directions were not clear for some of the parts, such as training and validation accuracy. There are multiple accuracies we could use for the LogisticRegression so this was confusing to understand. Something motivating was using the machine learning models to understand the results and intepreting it. It was cool seeing how coding could be used to interpret results in a way that can apply to real world situations.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db951b3a",
   "metadata": {},
   "source": [
    "## Part 5: Bonus Question (4 marks)\n",
    "\n",
    "Repeat Part 2 with Ridge and Lasso regression to see if you can improve the accuracy results. Which method and what value of alpha gave you the best R^2 score? Is this score \"good enough\"? Explain why or why not.\n",
    "\n",
    "**Remember**: Only test values of alpha from 0.001 to 100 along the logorithmic scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "47623d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression:\n",
      "Best alpha: 100.0\n",
      "Test MSE: 95.62517337012183\n",
      "Test R2 Score: 0.6369366906855762\n",
      "\n",
      "Lasso Regression:\n",
      "Best alpha: 9.770099572992246\n",
      "Test MSE: 95.11511718456465\n",
      "Test R2 Score: 0.638873238146232\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "alphas = np.logspace(-3, 2, num=100)  # Values from 0.001 to 100\n",
    "\n",
    "#RIDGE \n",
    "\n",
    "ridge_results = []\n",
    "\n",
    "for alpha in alphas:\n",
    "\n",
    "    ridge_model = Ridge(alpha=alpha)\n",
    "    ridge_model.fit(X_train, y_train)\n",
    "    \n",
    "   \n",
    "    test_predictions_ridge = ridge_model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    test_mse_ridge = mean_squared_error(y_test, test_predictions_ridge)\n",
    "    test_r2_ridge = r2_score(y_test, test_predictions_ridge)\n",
    "    \n",
    "    ridge_results.append((alpha, test_mse_ridge, test_r2_ridge))\n",
    "\n",
    "\n",
    "best_alpha_ridge, best_mse_ridge, best_r2_ridge = min(ridge_results, key=lambda x: x[1])\n",
    "\n",
    "print(\"Ridge Regression:\")\n",
    "print(f\"Best alpha: {best_alpha_ridge}\")\n",
    "print(f\"Test MSE: {best_mse_ridge}\")\n",
    "print(f\"Test R2 Score: {best_r2_ridge}\")\n",
    "print()\n",
    "\n",
    "#LASSO\n",
    "\n",
    "lasso_results = []\n",
    "\n",
    "for alpha in alphas:\n",
    "  \n",
    "    lasso_model = Lasso(alpha=alpha)\n",
    "    lasso_model.fit(X_train, y_train)\n",
    "    \n",
    "   \n",
    "    test_predictions_lasso = lasso_model.predict(X_test)\n",
    "    \n",
    "  \n",
    "    test_mse_lasso = mean_squared_error(y_test, test_predictions_lasso)\n",
    "    test_r2_lasso = r2_score(y_test, test_predictions_lasso)\n",
    "    \n",
    "    lasso_results.append((alpha, test_mse_lasso, test_r2_lasso))\n",
    "\n",
    "\n",
    "best_alpha_lasso, best_mse_lasso, best_r2_lasso = min(lasso_results, key=lambda x: x[1])\n",
    "\n",
    "print(\"Lasso Regression:\")\n",
    "print(f\"Best alpha: {best_alpha_lasso}\")\n",
    "print(f\"Test MSE: {best_mse_lasso}\")\n",
    "print(f\"Test R2 Score: {best_r2_lasso}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b606236",
   "metadata": {},
   "source": [
    "*ANSWER HERE* #The method which gave the best R2 score was using Lasso Regression, producing a 0.63887 score. The value of alpha was 9.77 as well. However, the MSE value was significantly higher than the linear by almost ten times. There was honestly no significant difference in the R2 score as well, and so this score of R2 is not good enough, since the model is still only 63.88% effective in matching the observed data points."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
