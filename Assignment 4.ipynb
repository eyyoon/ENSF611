{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 4: Pipelines and Hyperparameter Tuning (32 total marks)\n",
    "### Due: November 22 at 11:59pm\n",
    "\n",
    "### Name: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will be putting together everything you have learned so far. You will need to find your own dataset, do all the appropriate preprocessing, test different supervised learning models and evaluate the results. More details for each step can be found below.\n",
    "\n",
    "### You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf275ca7",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b67a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8219f163",
   "metadata": {},
   "source": [
    "## Step 1: Data Input (4 marks)\n",
    "\n",
    "Import the dataset you will be using. You can download the dataset onto your computer and read it in using pandas, or download it directly from the website. Answer the questions below about the dataset you selected. \n",
    "\n",
    "To find a dataset, you can use the resources listed in the notes. The dataset can be numerical, categorical, text-based or mixed. If you want help finding a particular dataset related to your interests, please email the instructor.\n",
    "\n",
    "**You cannot use a dataset that was used for a previous assignment or in class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2af8bd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8693, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>VIP</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>Name</th>\n",
       "      <th>Transported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>B/0/P</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>39.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Maham Ofracculy</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>F/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>24.0</td>\n",
       "      <td>False</td>\n",
       "      <td>109.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>Juanna Vines</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0003_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>58.0</td>\n",
       "      <td>True</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3576.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6715.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>Altark Susent</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0003_02</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>33.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>3329.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>Solam Susent</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0004_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>F/1/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>16.0</td>\n",
       "      <td>False</td>\n",
       "      <td>303.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Willy Santantines</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PassengerId HomePlanet CryoSleep  Cabin  Destination   Age    VIP  \\\n",
       "0     0001_01     Europa     False  B/0/P  TRAPPIST-1e  39.0  False   \n",
       "1     0002_01      Earth     False  F/0/S  TRAPPIST-1e  24.0  False   \n",
       "2     0003_01     Europa     False  A/0/S  TRAPPIST-1e  58.0   True   \n",
       "3     0003_02     Europa     False  A/0/S  TRAPPIST-1e  33.0  False   \n",
       "4     0004_01      Earth     False  F/1/S  TRAPPIST-1e  16.0  False   \n",
       "\n",
       "   RoomService  FoodCourt  ShoppingMall     Spa  VRDeck               Name  \\\n",
       "0          0.0        0.0           0.0     0.0     0.0    Maham Ofracculy   \n",
       "1        109.0        9.0          25.0   549.0    44.0       Juanna Vines   \n",
       "2         43.0     3576.0           0.0  6715.0    49.0      Altark Susent   \n",
       "3          0.0     1283.0         371.0  3329.0   193.0       Solam Susent   \n",
       "4        303.0       70.0         151.0   565.0     2.0  Willy Santantines   \n",
       "\n",
       "   Transported  \n",
       "0        False  \n",
       "1         True  \n",
       "2        False  \n",
       "3        False  \n",
       "4         True  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import dataset (1 mark)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc875ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20316765",
   "metadata": {},
   "source": [
    "### Questions (3 marks)\n",
    "\n",
    "1. (1 mark) What is the source of your dataset?\n",
    "1. (1 mark) Why did you pick this particular dataset?\n",
    "1. (1 mark) Was there anything challenging about finding a dataset that you wanted to use?\n",
    "\n",
    "*ANSWER HERE*\n",
    "\n",
    "1. The source of my dataset is Kaggle, the files were downloaded from the link: https://www.kaggle.com/competitions/spaceship-titanic/data?select=test.csv The files for train hat I loaded in using pandas will be provided with the assignment submission. \n",
    "2. I picked this particular dataset because I enjoy classification problems more (true, false); also, there are numerous categories to process such as categorical (Cryosleep, HomePlanet, VIP, etc.). There are also null values which need to be cleaned, so we can explore numerous data processing such as encoding and null value removal, and scaling. \n",
    "3. It was a little challenging because most of the class examples had examples of supervised learning, and it was hard finding datasets that were not unsupervised learning in Kaggle. However, I was able to understand that I could use the training set to model a supervised learning machine learning model! Since the assignment specifies: \"test different supervised learning models\", I was able to find a suitable dataset which I was interested in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e258b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42fea4cc",
   "metadata": {},
   "source": [
    "## Step 2: Data Processing (5 marks)\n",
    "\n",
    "The next step is to process your data. Implement the following steps as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afc244d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate values in 'ID' column: 0\n",
      "PassengerId       0\n",
      "HomePlanet      201\n",
      "CryoSleep       217\n",
      "Cabin           199\n",
      "Destination     182\n",
      "Age             179\n",
      "VIP             203\n",
      "RoomService     181\n",
      "FoodCourt       183\n",
      "ShoppingMall    208\n",
      "Spa             183\n",
      "VRDeck          188\n",
      "Name            200\n",
      "Transported       0\n",
      "dtype: int64\n",
      "PassengerId     0\n",
      "HomePlanet      0\n",
      "CryoSleep       0\n",
      "Cabin           0\n",
      "Destination     0\n",
      "Age             0\n",
      "VIP             0\n",
      "RoomService     0\n",
      "FoodCourt       0\n",
      "ShoppingMall    0\n",
      "Spa             0\n",
      "VRDeck          0\n",
      "Transported     0\n",
      "dtype: int64\n",
      "(7559, 13)\n"
     ]
    }
   ],
   "source": [
    "# Clean data (if needed)\n",
    "\n",
    "# Check if the 'ID' column has duplicate values since it is the primary key\n",
    "is_duplicate = df['PassengerId'].duplicated() #since there are none, don't need to clean duplicates\n",
    "duplicate_count = is_duplicate.sum()\n",
    "print(f\"Number of duplicate values in 'ID' column: {duplicate_count}\")\n",
    "\n",
    "print(df.isnull().sum()) #since there are quite a bit of nulls, we will need to clean it up. \n",
    "\n",
    "#dropping rows with missing values for the following;\n",
    "df = df.dropna(subset=['Name', 'Destination', 'HomePlanet', 'CryoSleep', 'Cabin', 'VIP']) \n",
    "\n",
    "#dropping columns which provide no useful correlation of being transported, i.e. Name \n",
    "df = df.drop('Name', axis=1)\n",
    "\n",
    "# Filling missing values in specific columns with the mean of each column\n",
    "columns_to_fill = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "\n",
    "for column in columns_to_fill:\n",
    "    df[column].fillna(df[column].mean(), inplace=True)\n",
    "\n",
    "print(df.isnull().sum()) #no nulls, cleaned up! \n",
    "print(df.shape) ##Shape was originally (8693, 14), so around ~900 rows were deleted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70a8c127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>VIP</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>Transported</th>\n",
       "      <th>Cabin_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>39.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>24.0</td>\n",
       "      <td>False</td>\n",
       "      <td>109.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0003_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>58.0</td>\n",
       "      <td>True</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3576.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6715.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0003_02</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>33.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>3329.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0004_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>16.0</td>\n",
       "      <td>False</td>\n",
       "      <td>303.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PassengerId HomePlanet CryoSleep  Destination   Age    VIP  RoomService  \\\n",
       "0     0001_01     Europa     False  TRAPPIST-1e  39.0  False          0.0   \n",
       "1     0002_01      Earth     False  TRAPPIST-1e  24.0  False        109.0   \n",
       "2     0003_01     Europa     False  TRAPPIST-1e  58.0   True         43.0   \n",
       "3     0003_02     Europa     False  TRAPPIST-1e  33.0  False          0.0   \n",
       "4     0004_01      Earth     False  TRAPPIST-1e  16.0  False        303.0   \n",
       "\n",
       "   FoodCourt  ShoppingMall     Spa  VRDeck  Transported  Cabin_encoded  \n",
       "0        0.0           0.0     0.0     0.0        False              1  \n",
       "1        9.0          25.0   549.0    44.0         True              1  \n",
       "2     3576.0           0.0  6715.0    49.0        False              2  \n",
       "3     1283.0         371.0  3329.0   193.0        False              2  \n",
       "4       70.0         151.0   565.0     2.0         True              1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implement preprocessing steps. Remember to use ColumnTransformer if more than one preprocessing method is needed\n",
    "\n",
    "#preprocessing of 'Cabin' as the values are too unique for any reliable encoding, however cabin may have a significant value\n",
    "#towards if people are transported. As a result, will need to preprocess this column using count encoding: \n",
    "\n",
    "frequency_map = df['Cabin'].value_counts().to_dict()\n",
    "df['Cabin_encoded'] = df['Cabin'].map(frequency_map)\n",
    "df = df.drop('Cabin', axis=1) #dropping Cabin now \n",
    "\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    [(\"scaling\", RobustScaler(), ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']),\n",
    "     (\"onehot\", OneHotEncoder(), ['Destination', 'HomePlanet', 'CryoSleep', 'VIP'])])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92c46b7",
   "metadata": {},
   "source": [
    "### Questions (2 marks)\n",
    "\n",
    "1. (1 mark) Were there any missing/null values in your dataset? If yes, how did you replace them and why? If no, describe how you would've replaced them and why.\n",
    "2. (1 mark) What type of data do you have? What preprocessing methods would you have to apply based on your data types?\n",
    "\n",
    "*ANSWER HERE*\n",
    "1. Yes, there were missing/null values. I dropped the row for missing values of 'Name', 'Destination', 'HomePlanet', 'CryoSleep', 'Cabin', 'VIP'. For name, it is mostly unique so filling with average does not make sense. Destination, HomePlanet, CryoSleep, Cabin and VIP also were dropped because there isn't really a clear majoriy for these categorized features, so filling with average also does not make sense. For columns 'Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', I filled missing values with the average mean of the respective columns as it is a continuous numerical value, so using the average would be a suitable way to replace. I also dropped the column 'Name' as it provides no meaningful correlation, and introduces high cardinality for the encoder. Lastly, I used count encoding to encode the column Cabin because there were too many unique values introduced. The value of this column is the amount of appearances in the cabin which can correlate to if people are transported, so I used count encoding and dropped the original column of 'Cabin'.\n",
    "\n",
    "2. I have both numerical and categorical types of data (features). This means that I will need to employ scaling, and I chose to use RobustScaler as upon a look, there are definitely data points or outliers different than the rest for these features. I will also have to encode the categorical features using OneHotEncoder as they are unordered. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a245d00",
   "metadata": {},
   "source": [
    "## Step 3: Implement Machine Learning Model (11 marks)\n",
    "\n",
    "In this section, you will implement three different supervised learning models (one linear and two non-linear) of your choice. You will use a pipeline to help you decide which model and hyperparameters work best. It is up to you to select what models to use and what hyperparameters to test. You can use the class examples for guidance. You must print out the best model parameters and results after the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5558a776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7559, 12)\n",
      "(7559,)\n",
      "(5291, 12)\n",
      "(5291,)\n",
      "Best params:\n",
      "{'logistic__C': 0.05, 'logistic__max_iter': 250}\n",
      "\n",
      "Best cross-validation score: 0.79\n",
      "Test-set score: 0.78\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Implement pipeline and grid search here. Can add more code blocks if necessary\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "logistic_pipeline = Pipeline([\n",
    "    ('preprocessor', ct),\n",
    "    ('logistic', LogisticRegression(solver='lbfgs'))  # Specify the solver explicitly\n",
    "])\n",
    "\n",
    "random_forest_pipeline = Pipeline([\n",
    "    ('preprocessor', ct),\n",
    "    ('random_forest', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "svm_pipeline = Pipeline([\n",
    "    ('preprocessor', ct),\n",
    "    ('svm', SVC())\n",
    "])\n",
    "\n",
    "# Define parameter grids for grid search\n",
    "logistic_params = {\n",
    "    'logistic__C': [0.1, .05, 1],\n",
    "    'logistic__max_iter': [250, 500, 1000]  # Increase the maximum number of iterations\n",
    "}\n",
    "\n",
    "random_forest_params = {\n",
    "    'random_forest__n_estimators': [2, 5, 10],\n",
    "    'random_forest__max_depth': [None, 2, 5],\n",
    "    'random_forest__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "svm_params = {\n",
    "    'svm__C': [0.1, 1, 10],\n",
    "    'svm__kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "# get all columns apart from transported for the features\n",
    "X = df.drop(columns='Transported')\n",
    "y = df['Transported']\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# split dataframe and transported\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                            test_size=0.3, stratify=y,random_state=0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "X_train.head()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "logistic_grid = GridSearchCV(logistic_pipeline, logistic_params, cv=5)\n",
    "logistic_grid.fit(X_train, y_train)\n",
    "\n",
    "rf_grid = GridSearchCV(random_forest_pipeline, random_forest_params, cv=5)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "svm_grid = GridSearchCV(svm_pipeline, svm_params, cv=5)\n",
    "svm_grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95dff2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Best params:\n",
      "{'logistic__C': 0.05, 'logistic__max_iter': 250}\n",
      "\n",
      "Best cross-validation score: 0.79\n",
      "\n",
      "\n",
      "Random Forests\n",
      "Best params:\n",
      "{'random_forest__max_depth': None, 'random_forest__min_samples_split': 10, 'random_forest__n_estimators': 10}\n",
      "\n",
      "Best cross-validation score: 0.78\n",
      "\n",
      "\n",
      "Support Vector Machine (Classification)\n",
      "Best params:\n",
      "{'svm__C': 1, 'svm__kernel': 'rbf'}\n",
      "\n",
      "Best cross-validation score: 0.79\n"
     ]
    }
   ],
   "source": [
    "print('Logistic Regression')\n",
    "print(\"Best params:\\n{}\\n\".format(logistic_grid.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(logistic_grid.best_score_))\n",
    "print('\\n')\n",
    "\n",
    "print('Random Forests')\n",
    "print(\"Best params:\\n{}\\n\".format(rf_grid.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(rf_grid.best_score_))\n",
    "print('\\n')\n",
    "\n",
    "print(\"Support Vector Machine (Classification)\")\n",
    "print(\"Best params:\\n{}\\n\".format(svm_grid.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(svm_grid.best_score_))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd7075",
   "metadata": {},
   "source": [
    "### Questions (5 marks)\n",
    "\n",
    "1. (1 mark) Do you need regression or classification models for your dataset?\n",
    "1. (2 marks) Which models did you select for testing and why?\n",
    "1. (2 marks) Which model worked the best? Does this make sense based on the theory discussed in the course and the context of your dataset?\n",
    "\n",
    "*ANSWER HERE*\n",
    "1. This dataset needs classification models because the target is either a true or false; there is no numerical value associated with the result.\n",
    "2. I selected the linear classification model of linear regression, as well as the non-linear classification models of random forest and SVC. These were all chosen respectively as a result of classification being the target result, and since the data is a binary classification problem. \n",
    "3. The model that worked the best was support vector machines and logistic regression based on the cross validcation score. Within the context of my dataset which is a binary classification problem, and my data is around ~8000 entries which is not large. I think my dataset prefers a simpler model which is less complex as given by the parameters of logistic regression, using a C value of 0.05 and iteration of 250 which means more generalization. Because the data prefers less complex models, random forests gave the worst score. However, I think SVC was able to also have a higher score as it works well on a variety of datasets. Based on theory I believe SVM should be the best performer since we are able to tune the parameters with grid search, but I think in reality logistic regression kept up because of the simplicity of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f994e31",
   "metadata": {},
   "source": [
    "## Step 4: Validate Model (6 marks)\n",
    "\n",
    "Use the testing set to calculate the testing accuracy for the best model determined in Step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69e64c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score - Logistic Regression: 0.78\n",
      "F1 Score - Random Forest: 0.76\n",
      "F1 Score - Support Vector Machine: 0.80\n"
     ]
    }
   ],
   "source": [
    "# Calculate testing accuracy (1 mark)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "# Logistic Regression\n",
    "logistic_predictions = logistic_grid.predict(X_test)\n",
    "f1_logistic = f1_score(y_test, logistic_predictions)\n",
    "\n",
    "# Random Forest\n",
    "rf_predictions = rf_grid.predict(X_test)\n",
    "f1_rf = f1_score(y_test, rf_predictions)\n",
    "\n",
    "# Support Vector Machine\n",
    "svm_predictions = svm_grid.predict(X_test)\n",
    "f1_svm = f1_score(y_test, svm_predictions)\n",
    "\n",
    "print(\"F1 Score - Logistic Regression: {:.2f}\".format(f1_logistic))\n",
    "print(\"F1 Score - Random Forest: {:.2f}\".format(f1_rf))\n",
    "print(\"F1 Score - Support Vector Machine: {:.2f}\".format(f1_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4529ba",
   "metadata": {},
   "source": [
    "\n",
    "### Questions (5 marks)\n",
    "\n",
    "1. (1 mark) Which accuracy metric did you choose? \n",
    "1. (1 mark) How do these results compare to those in part 3? Did this model generalize well?\n",
    "1. (3 marks) Based on your results and the context of your dataset, did the best model perform \"well enough\" to be used out in the real-world? Why or why not? Do you have any suggestions for how you could improve this analysis?\n",
    "\n",
    "*ANSWER HERE*\n",
    "1. I used the accuracy metric of F1 score because it combines precision and recall into a single value, providing a balance between these two metrics. Because it is a binary classification, the true and false is the result; it is important to capture the false positives and true negatives as a result.\n",
    "\n",
    "2. The results are fairly close, differing only by 0.1-0.2. This model as a result generalized well, but there is also the possibility of underfitting as the validation and training scores are similar. There is no overfitting as the test score is not significantly lower than the training set. I believe however it generalized well because using the gridsearch, it would have preferred to get the most complex c in logistic regression (1 vs 0.05) and also 10 instead of 1 for SVM. \n",
    "\n",
    "3. I believe for the context of my dataset, it did perform \"well enough\". There is no glaring signs of overfitting or underfitting based on the comparison of F1 score of the test set to the cross validation score of the training set. When we had gridsearch, it did not reach for the least generalization, indicating that the model is generalizing well. The model is able to consistently hit a similar score to the training dataset upon finding new datasets, at an accuracy of 80% using the SVM. If I was worried being transported to a different planet, I would definitely take my chances on this model! A suggestion that I could do to improve this analysis is get more data storage so that I can test more model parameters, as I had to wait ~30 min for the one on the notebook to work, even reducing the parameters as much as possible. I could also test out some other models such as gradient boosted decision trees. All in all, I am quite proud of this model analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b238f4",
   "metadata": {},
   "source": [
    "## Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93097bfe",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*\n",
    "\n",
    "1. I sourced my code using examples provided in class, such as ApplyPipelines located and Imputation Example2 both found in D2L: https://d2l.ucalgary.ca/d2l/le/content/543310/Home?itemIdentifier=TOC . \n",
    "\n",
    "2. I completed the steps as provided by this assignment. The steps were laid out very organized, I also did some pre-testing of the splitting of the dataset to confirm supervised learning. \n",
    "\n",
    "3. I did not use generative AI to modify the code at all. However, I used it to inform the methods that needed to be called, such as RobustScaler() and other methods such as count encoding. \n",
    "\n",
    "4. A challenge I had was trying to find a suitable dataset as well as trying to get used to performing a full machine learning (supervised) model all on my own. It was definitely a unique assignment where you are kind of left to your devices, so it was hard finding a dataset and thinking about which machine learning models I wanted to implement. I think what helped me to be successful was following the Imputation Example2 document on D2L to understand the processes I needed to follow. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd97b6ac",
   "metadata": {},
   "source": [
    "## Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challenging, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*\n",
    "\n",
    "I really enjoyed how this assignment gives the creative freedom to the student. It was definitely a new challenge to employ all the things I learned to a real-case machine learning model. There is direction to follow, however a lot of the steps such as preprocessing, cleaning, dataset picking and machine learning model implementation is left to the student so it was very enjoyable. I think I learned a lot more from this assignment than the previous ones, and I really enjoyed it as a result. This was a motivating assignment because it translated the class to the real world, and kind of enlightened me to the possibilities of using this class knowledge for implementing actual machine learning models on my own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241c3b12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
