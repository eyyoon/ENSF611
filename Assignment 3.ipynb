{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92778525",
   "metadata": {},
   "source": [
    "# Assignment 3: Non-Linear Models and Validation Metrics (37 total marks)\n",
    "### Due: October 24 at 11:59pm\n",
    "\n",
    "### Name: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will need to write code that uses non-linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf275ca7",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b67a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee2d2c3",
   "metadata": {},
   "source": [
    "## Part 1: Regression (14.5 marks)\n",
    "\n",
    "For this section, we will be continuing with the concrete example from yellowbrick. You will need to compare these results to the results from the previous assignment. Please use the results from the solution if you were unable to complete Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8219f163",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (0.5 marks)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
    "\n",
    "Use the yellowbrick function `load_concrete()` to load the concrete dataset into the feature matrix `X` and target vector `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2af8bd32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X: (1030, 8), Type of X: <class 'pandas.core.frame.DataFrame'>\n",
      "Size of y: (1030,), Type of y: <class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cement</th>\n",
       "      <th>slag</th>\n",
       "      <th>ash</th>\n",
       "      <th>water</th>\n",
       "      <th>splast</th>\n",
       "      <th>coarse</th>\n",
       "      <th>fine</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cement   slag  ash  water  splast  coarse   fine  age\n",
       "0   540.0    0.0  0.0  162.0     2.5  1040.0  676.0   28\n",
       "1   540.0    0.0  0.0  162.0     2.5  1055.0  676.0   28\n",
       "2   332.5  142.5  0.0  228.0     0.0   932.0  594.0  270\n",
       "3   332.5  142.5  0.0  228.0     0.0   932.0  594.0  365\n",
       "4   198.6  132.4  0.0  192.0     0.0   978.4  825.5  360"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: Import concrete dataset from yellowbrick library\n",
    "\n",
    "from yellowbrick.datasets.loaders import load_concrete\n",
    "\n",
    "concrete_data = load_concrete(return_dataset=True)\n",
    "\n",
    "X, y = load_concrete()\n",
    "print(f\"Size of X: {X.shape}, Type of X: {type(X)}\")\n",
    "print(f\"Size of y: {y.shape}, Type of y: {type(y)}\")\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fea4cc",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (0 marks)\n",
    "\n",
    "Data processing was completed in the previous assignment. No need to repeat here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a245d00",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import the Decision Tree, Random Forest and Gradient Boosting Machines regression models from sklearn\n",
    "2. Instantiate the three models with `max_depth = 5`. Are there any other parameters that you will need to set?\n",
    "3. Implement each machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f994e31",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model\n",
    "\n",
    "Calculate the average training and validation accuracy using mean squared error with cross-validation. To do this, you will need to set `scoring='neg_mean_squared_error'` in your `cross_validate` function and negate the results (multiply by -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc3f7a8",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: DT, RF and GB\n",
    "2. Add the accuracy results to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fdc93a78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Validation accuracy  Training accuracy\n",
      "Decision Tree               106.089609          32.287596\n",
      "Random Forest                62.665779          21.943341\n",
      "Gradient Boosting            55.352024           0.165123\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Step 3: Instantiate the models with max_depth = 5\n",
    "decision_tree = DecisionTreeRegressor(max_depth=5)\n",
    "random_forest = RandomForestRegressor(max_depth=5)\n",
    "gradient_boosting = GradientBoostingRegressor(max_depth=5)\n",
    "\n",
    "\n",
    "decision_tree.fit(X, y)\n",
    "random_forest.fit(X, y)\n",
    "gradient_boosting.fit(X, y)\n",
    "\n",
    "# Step 4/5: Calculate and print the results\n",
    "\n",
    "results = pd.DataFrame()\n",
    "\n",
    "models = [decision_tree, random_forest, gradient_boosting]\n",
    "model_names = [\"Decision Tree\", \"Random Forest\", \"Gradient Boosting\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Loop through each model and calculate training and validation accuracies\n",
    "for model, model_name in zip(models, model_names):\n",
    "  \n",
    "    cv_results = cross_validate(model, X_test, y_test, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "    \n",
    "   \n",
    "    mse_scores = -cv_results['test_score']\n",
    "    validation_accuracy = mse_scores.mean()\n",
    "    \n",
    "  \n",
    "    training_scores = -cv_results['train_score']\n",
    "    training_accuracy = training_scores.mean()\n",
    "    \n",
    "    # Add the results to the DataFrame\n",
    "    results.at[model_name, 'Validation accuracy'] = validation_accuracy\n",
    "    results.at[model_name, 'Training accuracy'] = training_accuracy\n",
    "    \n",
    "\n",
    "# Print the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31715a9d",
   "metadata": {},
   "source": [
    "Repeat the step above to print the R2 score instead of the mean-squared error. For this case, you can use `scoring='r2'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "83539f47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Validation R2 Score  Training R2 Score\n",
      "Decision Tree                 0.583812           0.877019\n",
      "Random Forest                 0.751915           0.917951\n",
      "Gradient Boosting             0.784500           0.999367\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "\n",
    "# Step 3: Instantiate the models with max_depth = 5\n",
    "decision_tree = DecisionTreeRegressor(max_depth=5)\n",
    "random_forest = RandomForestRegressor(max_depth=5)\n",
    "gradient_boosting = GradientBoostingRegressor(max_depth=5)\n",
    "\n",
    "\n",
    "decision_tree.fit(X, y)\n",
    "random_forest.fit(X, y)\n",
    "gradient_boosting.fit(X, y)\n",
    "\n",
    "results = pd.DataFrame()\n",
    "\n",
    "models = [decision_tree, random_forest, gradient_boosting]\n",
    "model_names = [\"Decision Tree\", \"Random Forest\", \"Gradient Boosting\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "for model, model_name in zip(models, model_names):\n",
    "    # Calculate R2 score using cross_validate\n",
    "    cv_results = cross_validate(model, X_test, y_test, cv=5, scoring='r2', return_train_score=True)\n",
    "    \n",
    "  \n",
    "    r2_scores = cv_results['test_score']\n",
    "    validation_accuracy = r2_scores.mean()\n",
    "    \n",
    "  \n",
    "    training_scores = cv_results['train_score']\n",
    "    training_accuracy = training_scores.mean()\n",
    "    \n",
    "    # Add the results to the DataFrame\n",
    "    results.at[model_name, 'Validation R2 Score'] = validation_accuracy\n",
    "    results.at[model_name, 'Training R2 Score'] = training_accuracy\n",
    "\n",
    "# Print the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5257a98",
   "metadata": {},
   "source": [
    "### Questions (6 marks)\n",
    "1. How do these results compare to the results using a linear model in the previous assignment? Use values.\n",
    "\n",
    "From Assignment 2 Solutions, the results were as follows from a linear model: \n",
    "\n",
    "       Validation accuracy\tTraining accuracy\n",
    "MSE\t     95.904136              111.358439\t           \n",
    "R2 score   0.623414\t             0.610823\n",
    "\n",
    "MSE:\n",
    "\n",
    "                    Validation accuracy  Training accuracy\n",
    "Decision Tree               106.089609          32.287596\n",
    "Random Forest                62.665779          21.943341\n",
    "Gradient Boosting            55.352024           0.165123\n",
    "\n",
    "R2:\n",
    "\n",
    "                      Validation R2 Score  Training R2 Score\n",
    "Decision Tree                 0.583812           0.877019\n",
    "Random Forest                 0.751915           0.917951\n",
    "Gradient Boosting             0.784500           0.999367\n",
    "\n",
    "We can directly compare the two results, and we can see that except from Decision Tree which performed slightly worse in the test data, Random Forest and Gradient Boosting both have better MSE and R2 scores when compared to the linear model. For example, the MSE of linear model is 95 for the test set whereas Random Forest boosts a 65, and a significant decrease to 21 for the training set. \n",
    "\n",
    "2. Out of the models you tested, which model would you select for this dataset and why?\n",
    "\n",
    "I would use the Gradient Boosting model because the validation accuracy and training accuracy in terms of MSE is the lowest for the dataset. Furthermore, the R2 score is relatively the best for this model as closer to 1 the R2 score is, the better performing the model is. With a .99 R2 score for training and .78 for the test set, we can see that this model performs the best. \n",
    "3. If you wanted to increase the accuracy of the tree-based models, what would you do? Provide two suggestions.\n",
    "We could identify the most relevant features by analyzing feature importance scores, such as those from Random Forest or Gradient Boosting models. Focusing on the features that contribute most to predictive power would increase the accuracy of these models. \n",
    "Another thing we could do is address data quality issues by handling missing values and outliers, and impute missing data or remove outliers to ensure a cleaner dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b238f4",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "\n",
    "I sourced my code using the previous assignment2 to import the concrete, and prepare the train/test datasets. I also used the help of the Binary Trees example:  https://d2l.ucalgary.ca/d2l/le/content/543310/viewContent/6097041/View\n",
    "This helped me understand which errors to get in cross validation. \n",
    "1. In what order did you complete the steps?\n",
    "I completed the steps as provided in the assignment, quite linearily. First, I imported the code from assignment2 to import concrete data, then instantiated the three models. I then cross validated the test set once split, and used the scoring system of MSE or R2 respectively to get the results. I appended these results to a dataframe, resulting in a completed Dataframe with validation and training accuracy. Then I created the loop of model type to get a total result for each type of model. \n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "I did not use generative AI for this portion of the code. It was very similar process to the last assignment, so using it as reference I was able to calculate the two errors quite well. \n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?\n",
    "No challenges, What really helped me to be successful was using Assignment 2 as reference as a lot of the processes was the same for this code. I think understanding the errors helped me ot interpolate the results as well and determine if the results were looking correct or incorrect, i.e. if MSE was 1000000 it would most likely be wrong!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93097bfe",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6de86",
   "metadata": {},
   "source": [
    "## Part 2: Classification (17.5 marks)\n",
    "\n",
    "You have been asked to develop code that can help the user classify different wine samples. Following the machine learning workflow described in class, write the relevant code in each of the steps below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d33a8",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (2 marks)\n",
    "\n",
    "The data used for this task can be downloaded from UCI: https://archive.ics.uci.edu/dataset/109/wine\n",
    "\n",
    "Use the pandas library to load the dataset. You must define the column headers if they are not included in the dataset \n",
    "\n",
    "You will need to split the dataset into feature matrix `X` and target vector `y`. Which column represents the target vector?\n",
    "\n",
    "Print the size and type of `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "33583c67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TO DO: Import wine dataset  \n",
    "\n",
    "# Define the column headers\n",
    "column_headers = [\n",
    "    \"Class\",\n",
    "    \"Alcohol\",\n",
    "    \"Malic_Acid\",\n",
    "    \"Ash\",\n",
    "    \"Alcalinity_of_Ash\",\n",
    "    \"Magnesium\",\n",
    "    \"Total_Phenols\",\n",
    "    \"Flavanoids\",\n",
    "    \"Nonflavanoid_Phenols\",\n",
    "    \"Proanthocyanins\",\n",
    "    \"Color_Intensity\",\n",
    "    \"Hue\",\n",
    "    \"OD280_OD315_of_Diluted_Wines\",\n",
    "    \"Proline\",\n",
    "]\n",
    "\n",
    "# Load the dataset\n",
    "wine_data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\", names=column_headers)\n",
    "\n",
    "# Split the dataset into feature matrix X and target vector y\n",
    "X = wine_data.drop(\"Class\", axis=1)  # Features\n",
    "y = wine_data[\"Class\"]  # Target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156db208",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (1.5 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28af110",
   "metadata": {},
   "source": [
    "Print the first five rows of the dataset to inspect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea266921",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Class  Alcohol  Malic_Acid   Ash  Alcalinity_of_Ash  Magnesium  \\\n",
      "0      1    14.23        1.71  2.43               15.6        127   \n",
      "1      1    13.20        1.78  2.14               11.2        100   \n",
      "2      1    13.16        2.36  2.67               18.6        101   \n",
      "3      1    14.37        1.95  2.50               16.8        113   \n",
      "4      1    13.24        2.59  2.87               21.0        118   \n",
      "\n",
      "   Total_Phenols  Flavanoids  Nonflavanoid_Phenols  Proanthocyanins  \\\n",
      "0           2.80        3.06                  0.28             2.29   \n",
      "1           2.65        2.76                  0.26             1.28   \n",
      "2           2.80        3.24                  0.30             2.81   \n",
      "3           3.85        3.49                  0.24             2.18   \n",
      "4           2.80        2.69                  0.39             1.82   \n",
      "\n",
      "   Color_Intensity   Hue  OD280_OD315_of_Diluted_Wines  Proline  \n",
      "0             5.64  1.04                          3.92     1065  \n",
      "1             4.38  1.05                          3.40     1050  \n",
      "2             5.68  1.03                          3.17     1185  \n",
      "3             7.80  0.86                          3.45     1480  \n",
      "4             4.32  1.04                          2.93      735  \n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "\n",
    "# Print the first five rows of the dataset\n",
    "print(wine_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834fc8fe",
   "metadata": {},
   "source": [
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "97c6e9dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      " Class                           0\n",
      "Alcohol                         0\n",
      "Malic_Acid                      0\n",
      "Ash                             0\n",
      "Alcalinity_of_Ash               0\n",
      "Magnesium                       0\n",
      "Total_Phenols                   0\n",
      "Flavanoids                      0\n",
      "Nonflavanoid_Phenols            0\n",
      "Proanthocyanins                 0\n",
      "Color_Intensity                 0\n",
      "Hue                             0\n",
      "OD280_OD315_of_Diluted_Wines    0\n",
      "Proline                         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "\n",
    "missing_values = wine_data.isnull().sum()\n",
    "\n",
    "# Print the count of missing values for each column\n",
    "print(\"Missing Values:\\n\", missing_values)\n",
    "\n",
    "#Since there is no missing values, no appropriate method to fill in missing values required. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070956af",
   "metadata": {},
   "source": [
    "How many samples do we have of each type of wine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b37a6fd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples for Each Type of Wine:\n",
      "2    71\n",
      "1    59\n",
      "3    48\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "\n",
    "# Count the number of samples for each type of wine\n",
    "wine_counts = wine_data[\"Class\"].value_counts()\n",
    "\n",
    "# Print the counts\n",
    "print(\"Number of Samples for Each Type of Wine:\")\n",
    "print(wine_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6c46f",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import `SVC` and `DecisionTreeClassifier` from sklearn\n",
    "2. Instantiate models as `SVC()` and `DecisionTreeClassifier(max_depth = 3)`\n",
    "3. Implement the machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0870b0d2",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model \n",
    "\n",
    "Calculate the average training and validation accuracy using `cross_validate` for the two different models listed in Step 3. For this case, use `scoring='accuracy'`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0bbd83",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "#### Step 5.1: Compare Models\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy\n",
    "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "be4b5c0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Method  Data Size  Training Accuracy  Validation Accuracy\n",
      "0            SVC        178           0.805419             0.803571\n",
      "1  Decision Tree        178           1.000000             0.832143\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
    "\n",
    "#Step 3\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "svc_model = SVC()\n",
    "dt_model = DecisionTreeClassifier(max_depth=3)\n",
    "svc_model.fit(X, y)\n",
    "dt_model.fit(X, y)   \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Step 4/5\n",
    "results = pd.DataFrame(columns=['Method', 'Data Size', 'Training Accuracy', 'Validation Accuracy'])\n",
    "\n",
    "data_sizes = []\n",
    "training_accuracies = []\n",
    "validation_accuracies = []\n",
    "methods = []\n",
    "\n",
    "# Calculate the data size, training accuracy, and validation accuracy for each model\n",
    "for model, model_name in [(svc_model, 'SVC'), (dt_model, 'Decision Tree')]:\n",
    "    scores = cross_validate(model, X_test, y_test, cv=5, scoring='accuracy', return_train_score=True)\n",
    "    \n",
    " \n",
    "    data_size = X.shape[0]\n",
    "    \n",
    " \n",
    "    train_accuracy = scores['train_score'].mean()\n",
    "    validation_accuracy = scores['test_score'].mean()\n",
    "    \n",
    "  \n",
    "    data_sizes.append(data_size)\n",
    "    training_accuracies.append(train_accuracy)\n",
    "    validation_accuracies.append(validation_accuracy)\n",
    "    methods.append(model_name)\n",
    "\n",
    "# Add the data to the DataFrame\n",
    "results['Method'] = methods\n",
    "results['Data Size'] = data_sizes\n",
    "results['Training Accuracy'] = training_accuracies\n",
    "results['Validation Accuracy'] = validation_accuracies\n",
    "\n",
    "# Print the results\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e17878",
   "metadata": {},
   "source": [
    "#### Step 5.2: Visualize Classification Errors\n",
    "Which method gave the highest accuracy? Use this method to print the confusion matrix and classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "44b091a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_depth=3)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO: Implement best model\n",
    "\n",
    "highest_accuracy_method = results.loc[results['Validation Accuracy'].idxmax()]['Method']\n",
    "\n",
    "print(highest_accuracy_method)\n",
    "dt_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "09d21b59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TO DO: Print confusion matrix using a heatmap\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make predictions\n",
    "y_pred = dt_model.predict(X_test)\n",
    "    \n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "# Print confusion matrix using a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5ef95947",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      1.00      0.97        14\n",
      "           2       1.00      0.94      0.97        16\n",
      "           3       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           0.97        36\n",
      "   macro avg       0.98      0.98      0.98        36\n",
      "weighted avg       0.97      0.97      0.97        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TO DO: Print classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf319621",
   "metadata": {},
   "source": [
    "### Questions (6 marks)\n",
    "1. How do the training and validation accuracy change depending on the method used? Explain with values.\n",
    "\n",
    "           Method  Data Size  Training Accuracy  Validation Accuracy\n",
    "0            SVC        178           0.805419             0.803571\n",
    "1  Decision Tree        178           1.000000             0.832143\n",
    "\n",
    "From the results of training and validation accuracy, we can see that using SVC resulted in a worse training accuracy as well as validation accuracy (.805 vs 1)! The validation accuracy is .032 greater as well, showing us that Decision Tree is a better model for this dataset. \n",
    "\n",
    "1. What are two reasons why the support vector machines model did not work as well as the tree-based model?\n",
    "SVMs don’t scale very well with the number of samples, so running this data with 14x178 may have caused issues. Another downside of SVMs is that they require careful preprocessing of the data and tuning of the parameters, and because we didn't really do this for this dataset, it may have cause more errors. As a result, SVM's are more sensitive to noisy or outlier data points causing inaccuracies. Lastly, SVMs are designed to find linear decision boundaries in the feature space. SVMs may struggle to capture a non-linearity relationship effectively as a result, whereas Decision trees are more suitable for non-linear relationships in the data.\n",
    "\n",
    "\n",
    "2. How many samples were incorrectly classified in step 5.2? \n",
    "In order to determine incorrectly classified samples, we used the accuracy score in the classification report 0.97.\n",
    "\n",
    "Number of Incorrectly Classified Samples = Total Samples - (Accuracy * Total Samples)\n",
    "\n",
    "Total Samples: 36 (as indicated in the \"support\" of the classification report)\n",
    "Accuracy: 0.97 (as the overall accuracy from the report)\n",
    "Number of Incorrectly Classified Samples = 36 - (0.97 * 36) =  0.48\n",
    "\n",
    "Rounding up to a whole number, we can deduce that the number of incorrectly classified samples was 1. \n",
    "\n",
    "3. In this case, is maximizing precision or recall more important? Why?\n",
    "\n",
    "Precision measures the accuracy of positive predictions. In this context, high precision means that when the model predicts a class, it is very likely to be correct. Recall measures the ability of the model to capture all positive instances. High recall means that the model is effective at identifying most of the positive instances. With two perfect precision scores for class 2 and 3, and 0.93 for class 1, it is all very high, however since relatively 0.93 is lower than 0.94 it may be better to maximize precision. However, since these results are very close, there is really an argument for both sides.  \n",
    "\n",
    "\n",
    "*YOUR ANSWERS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664ff8ae",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "\n",
    "I sourced my code using the first part of this assignment and more analysis to the data given. For example, the column headers was provided in the wine.names file. Using the previous code in this assignment, I followed similar code however had to modify a few things, but overall was pretty fun. \n",
    "1. In what order did you complete the steps?\n",
    "I completed the steps as shown above, following the exact steps guided by the assignment. \n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "I did use generative AI to determine the formula to calculate the incorrectly classified samples in step 5.2, as I was not sure how to do this. \n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?\n",
    "\n",
    "I think the biggest challenge for me at this part was trying to make the classification matrix and classification report as I had to look to see how to do this. It wasn't readily available in the examples or labs so I had to do some digging. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e837da",
   "metadata": {},
   "source": [
    "*DESCRIBE YOUR PROCESS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd7358d",
   "metadata": {},
   "source": [
    "## Part 3: Observations/Interpretation (3 marks)\n",
    "\n",
    "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
    "\n",
    "The classification report shows that the model performs very well in classifying multiple classes. It achieves high precision and recall for most classes, reflecting its ability to make accurate predictions without sacrificing the ability to capture positive instances. The F1-scores are high, indicating a good balance between precision and recall. The overall accuracy is also impressive at 97%. These results align with the lecture discussions on the importance of precision, recall, balance, and evaluating model performance in multi-class classification.\n",
    "\n",
    "*ADD YOUR FINDINGS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd97b6ac",
   "metadata": {},
   "source": [
    "## Part 4: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*\n",
    "\n",
    "I overall enjoyed working on this assignment, and it made me realize that although there are different models we can code such as Decision Trees or SVM's, the process is almost identicial to linear regression. Theres like a step formula you can follow to instantiate these machine learning models and most of the time, it is very similar. The difficult part is interpolating or understanding the results and trying to improve the model in my opinion. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa21e53b",
   "metadata": {},
   "source": [
    "## Part 5: Bonus Question (3 marks)\n",
    "\n",
    "Repeat Part 2 and compare the support vector machines model used to `LinearSVC(max_iter=5000)`. Does using `LinearSVC` improve the results? Why or why not?\n",
    "\n",
    "Is `LinearSVC` a good fit for this dataset? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "30fea72e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Method  Data Size  Training Accuracy  Validation Accuracy\n",
      "0        SVC        178           0.805419             0.803571\n",
      "1  LinearSVC        178           0.930788             0.885714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ericy\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ericy\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ericy\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ericy\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ericy\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ericy\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ericy\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ericy\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ericy\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ericy\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# TO DO: ADD YOUR CODE HERE\n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "svc_model = SVC()\n",
    "linear_svc_model = LinearSVC(max_iter=5000)\n",
    "dt_model = DecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "\n",
    "models = [svc_model, linear_svc_model]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "results = pd.DataFrame(columns=['Method', 'Data Size', 'Training Accuracy', 'Validation Accuracy'])\n",
    "\n",
    "# Initialize lists to store the data\n",
    "data_sizes = []\n",
    "training_accuracies = []\n",
    "validation_accuracies = []\n",
    "methods = []\n",
    "\n",
    "# Calculate the data size, training accuracy, and validation accuracy for each model\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__  # Get the model name\n",
    "    scores = cross_validate(model, X_test, y_test, cv=5, scoring='accuracy', return_train_score=True)\n",
    "    \n",
    "\n",
    "    data_size = X.shape[0]\n",
    "    \n",
    "\n",
    "    train_accuracy = scores['train_score'].mean()\n",
    "    validation_accuracy = scores['test_score'].mean()\n",
    "    \n",
    "    # Append the results to the lists\n",
    "    data_sizes.append(data_size)\n",
    "    training_accuracies.append(train_accuracy)\n",
    "    validation_accuracies.append(validation_accuracy)\n",
    "    methods.append(model_name)\n",
    "\n",
    "# Add the data to the DataFrame\n",
    "results['Method'] = methods\n",
    "results['Data Size'] = data_sizes\n",
    "results['Training Accuracy'] = training_accuracies\n",
    "results['Validation Accuracy'] = validation_accuracies\n",
    "\n",
    "# Print the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabc68a4",
   "metadata": {},
   "source": [
    "*ANSWER HERE*\n",
    "\n",
    "With a better accuracy for the training and test sets, LinearSVC improves the results when compared to SVC. (0.803 vs 0.885) When compared to decision trees, the test accuracy is actually greater (0.88 vs 0.86) so there may be a point in using this as the most optimal model! However, since training accuracy is not 1, it is not as great compared to decision trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241c3b12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbef3fc-1ecf-482c-88c1-d84e9859c234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
